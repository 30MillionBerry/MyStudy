{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Ensemble,RandomForest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2noRupwthNzjjk2cnNjve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohyunyang/MyStudy/blob/master/ML_Ensemble%2CRandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정형데이터와 비정형 데이터\n",
        "- 정형데이터는 주로 프로그래머가 다루는 데이터로서, 어떤 구조로 되어 있음(CSV, Excel에 저장하기 쉬움)\n",
        "- 비정형데이터는 데이터베이스나 엑셀로 표현하기 어려움, 책의 글과 같은 텍스트데이터, 디지털카메라로 찍은 사진, 핸드폰으로 듣는 디지털음악 등등"
      ],
      "metadata": {
        "id": "x0oHojhH8Kv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble learning (앙상블 학습)\n",
        "- 정형데이터를 다루는데 가장 뛰어난 성과를 내는 알고리즘\n",
        "- 이 알고리즘은 대부분 결정트리를 기반함\n",
        "- 비정형데이터는 규칙성을 찾기 어려워 전통적인 머신러닝 방법으로는 모델을 만들기 까다로움, 해서 신경망 알고리즘으로 모델을 만들 수 있음)"
      ],
      "metadata": {
        "id": "EwKRiEV_8sFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest (랜덤 포레스트)\n",
        "- 앙상블 학습의 대표주자로 안정적인 성능 덕분에 널리 사용됨\n",
        "- 결정트리를 랜덤하게 만들어 결정트리(나무)의 숲을 만듦\n",
        "- 각 결정트리의 예측을 사용해 최종 예측을 만듦\n",
        "\n",
        "\n",
        "> Bootstrap sample (부트스트랩 샘플) : 훈련데이터에서 랜덤하게 샘플을 추출하여 훈련데이터를 만드는데 이 때 한 샘플이 중복될 수도 있음 ex) 1개를 뽑고 다시 가방에 넣은다음 다시 뽑음\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUZV8M9d9MjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = pd.read_csv('http://bit.ly/wine_csv_data')\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    data, target, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "xV_NPFUrWYwA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cross_validate() 함수를 사용해 교차검증 수행하기"
      ],
      "metadata": {
        "id": "eyKVKabNYHx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier     # RandomForestClassifier는 기본적으로 100개의 결정트리를 사용함\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42) # n_jobs = -1 로 설정하여 최대한 병렬로 교차검증 수행 / 모든 CPU 코어를 사용하는게 좋음\n",
        "scores = cross_validate(rf, train_input, train_target,  # return_train_score=True, 검증점수 뿐아니라 훈련세트에 대한 점수도 같이 반환함 (기본값은 False로 되있음)\n",
        "                       return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "# 훈련세트에 다소 과대적합한 결과(여기서는 알고리즘을 조사하는것이 목적이므로 매개변수를 더 조정하지 않도록하자)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOP3G3fYYOPS",
        "outputId": "5fa01a8f-7a3e-41e3-ad80-c95b296667fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_)\n",
        "\n",
        "# 랜덤포레스트의 특성중요도는 각 결정트리의 특성중요도를 취합한 것임\n",
        "# 다음은 랜덤포레스트 모델을 훈련세트에 훈련한 후 특성 중요도를 출력한 값이다.\n",
        "# [알콜도수, 당도, pH]\n",
        "# [0.12345626, 0.89892934,0.0079144] 다음은 결정트리의 특성중요도이다\n",
        "# 분석해보자면 당도의 중요도 감소하고 나머지는 상승했다.\n",
        "# 이런 이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정트리를 훈련하기 때문이다.\n",
        "# 결과 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다\n",
        "# 이는 과대적합을 줄이고 일반화 성능을 높이는데 도움된다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAMQuU-IY3sE",
        "outputId": "cd216524-a5de-4bd8-bdd1-d35e04b4222c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OOB (out of bag)\n",
        "부트스트랩 샘플에 포함되지 않고 남는 샘플\n",
        "- 이 남는 샘플은 부트스트랩 샘플로 훈련한 결정트리를 평가할 수 있음\n",
        "- 검증세트의 역할을 함\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BMGhaTRRb1Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)\n",
        "\n",
        "# 교차검증에서 얻은 점수와 매우 비슷한 결과를 얻음\n",
        "# oob 점수를 사용하면 교차검증을 대신할 수 있어서 결과적으로 훈련세트에 더 많은 샘플을 사용할 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY8z3SnkcKIz",
        "outputId": "9fd757d2-ba13-43a0-f0c4-8d07d57a99c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Trees (엑스트라 트리)\n",
        "- 기본적으로 100개의 결정트리를 훈련\n",
        "- 랜덤포레스트와 동일하게 결정트리가 제공하는 대부분의 매개변수를 지원\n",
        "- 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용\n",
        "- 랜덤포레스트와 엑스트라 트리의 차이점은 부트스트랩 샘플을 사용하지 않는다\n",
        "- 노드를 분할 할 때 가장 좋은 분할을 찾는 것이 아닌, 무작위로 분할 함"
      ],
      "metadata": {
        "id": "FrXDBsuzc7eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "score = cross_validate(et, train_input, train_target,\n",
        "                       return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(score['test_score']))\n",
        "\n",
        "# 보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤포레스트보다 더 많은 결정트리를 훈련해야함\n",
        "# 그러나 랜덤하게 노드를 분할 하기 때문에 빠른 계산속도가 장점이다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rNZZGeKdDwN",
        "outputId": "fa1baf7b-26d5-41a6-c1ae-4dc7e28b35ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtutQZ7HiL7I",
        "outputId": "bb30d595-74cd-456b-903f-e5c550ea56ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gradient boosting (그레이디언트 부스팅)\n",
        "깊이가 얕은 결정트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법\n",
        "- GrandientBoostingClassifier 는 기본적으로 깊이가 3인 결정트리를 100개 사용\n",
        "- 깊이가 얕은 결정트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화성능을 기대할 수 있음\n",
        "- 경사하강법을 사용하여 트리를 앙상블에 추가함 / 분류에서는 로지스틱 손실함수를, 회귀에서는 평균제곱오차함수를 사용\n",
        "- 경실하강법이 손실함수를 산으로 정의하고 가장 낮은 곳을 찾아 내려오는 과정(모델의 가중치와 절편을 조금씩바꾸며)이라면 그레이디언트 부스팅은 결정트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동함"
      ],
      "metadata": {
        "id": "5EZGIEahiYGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# 과대적합이 되지 않음. 그레이디언트 부스팅은 결정트리의 개수를 늘려도 과대적합에 매우 강함\n",
        "# 학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RphX7lvJiyDG",
        "outputId": "f4fffb26-05d2-4f27-9c56-2035b9de4b0d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2,  # 결정트리개수를 500개로 늘렸지만 과대적합을 잘 억제하는 중 # learning_rate=0.1 이 기본 값\n",
        "                                random_state=42)\n",
        "score = cross_validate(gb, train_input, train_target,\n",
        "                       return_train_score=True, n_jobs= -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLE0KEppk4yA",
        "outputId": "69f349d6-8d1d-4b9a-e4e7-83d5e444479b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WasWbAYFl0gp",
        "outputId": "8526cd65-c577-48b3-a0dc-a872906f3211"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histogram-based Gradient Boosting (히스토그램 기반 그레이디언트 부스팅)\n",
        "HistGradientBoostingClassifier\n",
        "- 정형데이터를 다루는 머신러닝 알고리즘 중 가장 인기\n",
        "- 히스토그램 기반 그레이디언트 부스팅은 먼저 입력특성을 256개 구간으로 나눔\n",
        "- 따라서 노드를 분할 할 때 최적의 분할을 매우 빠르게 찾을 수 있음\n",
        "- 히스토그램 기반 그레이디언트 부스팅은 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용\n",
        "- 따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리 할 필요가 없음\n",
        "- 기본 매개변수에서 안정적인 성능을 얻을 수 있음"
      ],
      "metadata": {
        "id": "yXbYMcFCnr27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target,\n",
        "                        return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhCJy5kqoZIX",
        "outputId": "13b6c84e-0fd8-4166-bca1-43f9aa788a6f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:17: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  \"Since version 1.0, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 과대적합을 잘 억제하면서 그레이디언트 부스팅보다 조금 더 높은 성능을 제공\n",
        "# 특성의 중요도 확인\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target,\n",
        "                                n_repeats=10, random_state=42, n_jobs= -1)  # n_repeats = 의 기본값은 5\n",
        "print(result.importances_mean)\n",
        "\n",
        "# importances, importances_mean, importances_std를 담고 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUJDkr1LpUbs",
        "outputId": "68497b2a-06d8-48e5-f6cd-5a8b61a6b6ff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = permutation_importance(hgb, test_input, test_target,\n",
        "                                n_repeats=10, random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "# 당도에 더 집중하고 있군..\n",
        "# 이런 모델을 실전에 투입했을 시 어떤 특성에 관심을 둘지 예상할 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjJEvFjCqiMt",
        "outputId": "c39f4d7f-fb9d-4c9f-f6a7-a65b924e0fb0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hgb.score(test_input, test_target) # 테스트세트에서 성능을 최종적으로 확인하기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQxJcLEGrB34",
        "outputId": "b4da8190-4189-4225-81f4-d178d4b1c8f5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb,train_input, train_target,\n",
        "                        return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96Fe8AGOxvUc",
        "outputId": "9bfcdedf-7535-4f2b-8b86-0282bdfe8281"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8824322471423747 0.8726214185237284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs= -1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA1j9W--zPT7",
        "outputId": "f05ff379-b638-4f20-cf2d-355e1560c809"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9338079582727165 0.8789710890649293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 싸이킷런\n",
        "1. 랜덤포레스트 : 부트스트랩 샘플사용, 대표 앙상블 학습 알고리즘\n",
        "\n",
        "> 성능이 좋고 안정적임, 랜덤포레스트는 결정트리를 훈련하기 위해 부트스트랩 샘플을 만들고 전체 특성 중 일부를 랜덤하게 선택하여 결정트리를 만듦\n",
        "\n",
        "2. 엑스트라 트리 : 결정트리의 노드를 랜덤하게 분할함\n",
        "\n",
        "> 렌덤포레스트와 매우 비슷하지만 부트스트랩샘플을 사용하지 않고 노드를 분할할 때 최선이 아니라 랜덤하게 분할함, 랜덤포레스트보다 훈련속도가 빠르지만 보통 더 많은 트리필요함 \n",
        "\n",
        "\n",
        "3. 그레이디언트 부스팅 : 이전 트리의 손실을 보완하는 식으로 얕은 결정트리를 연속하여 추가함\n",
        "\n",
        "\n",
        "> 깊이가 얕은 트리를 연속적으로 추가하여 손실함수를 최소화하는 앙상블방법이다. 성능은 뛰어나지만 병렬로 훈련할 수 없기에 랜덤포레스트나 엑스트라트리보다 훈련속도가 조금 느림, 학습률 매개변수를 조정하여 모델의 복잡도를 제어가능, 학습률 매개변수가 크면 복잡하고 훈련세트에 과대적합된 모델을 얻을 수 있다\n",
        "\n",
        "\n",
        "4. 히스토그램 기반 그레이디언트 부스팅 : 훈련데이터를 256개 정수구간으로 나누어 빠르고 높은 성능을 냄\n",
        "\n"
      ],
      "metadata": {
        "id": "Fc0G1syb0BM-"
      }
    }
  ]
}