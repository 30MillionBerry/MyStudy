{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NLP(Text_Preprocessing3).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzb4xq54xt5nSJIV/xJ/ag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohyunyang/MyStudy/blob/master/DL_NLP(Text_Preprocessing3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패딩(Padding)\n",
        "- 자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. \n",
        "- 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. \n",
        "- ★ 다시 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있습니다. 실습을 통해 이해해봅시다."
      ],
      "metadata": {
        "id": "hmtSTbeb9YZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "numpy로 패딩하기"
      ],
      "metadata": {
        "id": "SiXnxGg09cRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
        "\n",
        "# 단어 집합을 만들고, 정수 인코딩을 수행합니다.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences) \n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)\n",
        "\n",
        "# 단어의 최대 길이 확인\n",
        "max_len = max(len(item) for item in encoded)\n",
        "print('최대 길이:',max_len)"
      ],
      "metadata": {
        "id": "k-uHGjjh9d-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in encoded:\n",
        "    while len(sentence) < max_len:\n",
        "        sentence.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "metadata": {
        "id": "yzSf4G739e4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 케라스 전처리 도구로 패딩하기"
      ],
      "metadata": {
        "id": "dNG4t4wh9e7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "b8H6AM6d9e9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded1 = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded1)"
      ],
      "metadata": {
        "id": "eIG2hiwF9e_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(encoded1)\n",
        "padded"
      ],
      "metadata": {
        "id": "pYRFxK6z9fB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy로 패딩을 진행하였을 때와는 패딩 결과가 다른데 그 이유는 pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라 앞에 0으로 채우기 때문입니다. 뒤에 0을 채우고 싶다면 인자로 padding='post'를 주면됩니다.\n",
        "\n",
        "padded_b = pad_sequences(encoded1, padding='post')\n",
        "padded_b"
      ],
      "metadata": {
        "id": "r_SHIBFd9fEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy를 이용하여 패딩을 했을 때와 결과가 동일합니다. 실제로 결과가 동일한지 두 결과를 비교합니다.\n",
        "(padded_b == padded_np).all()"
      ],
      "metadata": {
        "id": "scR7R8t69fGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "길이가 5보다 짧은 문서들은 0으로 패딩되고, 기존에 5보다 길었다면 데이터가 손실됩니다. 가령, 뒤에서 두번째 문장은 본래 [ 7, 7, 3, 2, 10, 1, 11]였으나 현재는 [ 3, 2, 10, 1, 11]로 변경된 것을 볼 수 있습니다. 만약, 데이터가 손실될 경우에 앞의 단어가 아니라 뒤의 단어가 삭제되도록 하고싶다면 truncating이라는 인자를 사용합니다. truncating='post'를 사용할 경우 뒤의 단어가 삭제됩니다."
      ],
      "metadata": {
        "id": "oc282zaO9fI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_max = pad_sequences(encoded, padding='post', maxlen=5)\n",
        "padded_max"
      ],
      "metadata": {
        "id": "HgHcwa779fLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  truncating='post'를 사용할 경우 뒤의 단어가 삭제\n",
        "padded_max1 = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
        "padded_max1 truncating='post'를 사용할 경우 뒤의 단어가 삭제"
      ],
      "metadata": {
        "id": "Pp-EHvlQ9fNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 전처리 패키지"
      ],
      "metadata": {
        "id": "AW4VVTgL9fQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. PyKoSpacing"
      ],
      "metadata": {
        "id": "0TtWFvKM9fSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "metadata": {
        "id": "P_9z66vg9fUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'"
      ],
      "metadata": {
        "id": "yxlK_c6B9fXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent = sent.replace(\" \",'') # 띄어쓰기를 공백으로"
      ],
      "metadata": {
        "id": "X4RVz7T79v67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent"
      ],
      "metadata": {
        "id": "WETgNXxT9v-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from pykospacing import Spacing\n",
        "\n",
        "# tf.autograph.experimental.do_not_convert\n",
        "spacing = Spacing() # 띄어쓰기 객체\n",
        "kospacing_sent = spacing(new_sent)\n",
        "print(sent)\n",
        "print(kospacing_sent)"
      ],
      "metadata": {
        "id": "fGB1W4q79wBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Py-Hanspell"
      ],
      "metadata": {
        "id": "ML94Vt_B9wD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "metadata": {
        "id": "Z52AhMYN9wHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
        "spelled_sent = spell_checker.check(sent)\n",
        "\n",
        "hanspell_sent = spelled_sent.checked\n",
        "print(hanspell_sent)"
      ],
      "metadata": {
        "id": "oLkTWypc9wJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelled_sent = spell_checker.check(new_sent)\n",
        "\n",
        "hanspell_sent = spelled_sent.checked\n",
        "print(hanspell_sent)\n",
        "print(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과"
      ],
      "metadata": {
        "id": "Z9KiZjtj9wMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
        "sent_1 = \"므리커럭이 나와쑈 우뤼가 다 치오끄요 바퀴뽈레 섀끠들이 케쇽 나호니끄 그 뜸을 막흐세욧\""
      ],
      "metadata": {
        "id": "5ZRF88dJ9wOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SOYNLP를 이용한 단어 토큰화"
      ],
      "metadata": {
        "id": "yKMnBGtH9wRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "soynlp는 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저입니다. 비지도 학습으로 단어 토큰화를 한다는 특징을 갖고 있으며, 데이터에 자주 등장하는 단어들을 단어로 분석합니다. soynlp 단어 토크나이저는 내부적으로 단어 점수 표로 동작합니다. 이 점수는 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용합니다."
      ],
      "metadata": {
        "id": "U4rPyfwp9wbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "eG8X06fv9wfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "tokenizer = Okt()\n",
        "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정'))"
      ],
      "metadata": {
        "id": "LJfjKyDO9wjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install soynlp"
      ],
      "metadata": {
        "id": "VT02Q5q8-CJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from soynlp import DoublespaceLineCorpus\n",
        "from soynlp.word import WordExtractor\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")\n"
      ],
      "metadata": {
        "id": "KVU7D2xf-CL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터를 다수의 문서로 분리\n",
        "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
        "len(corpus)"
      ],
      "metadata": {
        "id": "pE1a8xar-COg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SOYNLP의 브랜칭 엔트로피(branching entropy)"
      ],
      "metadata": {
        "id": "lHRf1_Wa-CRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터를 다수의 문서로 분리\n",
        "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
        "len(corpus)\n",
        "\n",
        "i = 0\n",
        "for d in corpus:\n",
        "  if len(d) > 0:\n",
        "    print(i+1,d)\n",
        "    i += 1\n",
        "  if i == 10:\n",
        "    break\n",
        "\n",
        "print(type(corpus))"
      ],
      "metadata": {
        "id": "QpDcu0SG-CTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "정상 출력되는 것을 확인하였습니다. soynlp는 학습 기반의 단어 토크나이저이므로 기존의 KoNLPy에서 제공하는 형태소 분석기들과는 달리 학습 과정을 거쳐야 합니다. 이는 전체 코퍼스로부터 응집 확률과 브랜칭 엔트로피 단어 점수표를 만드는 과정입니다. WordExtractor.extract()를 통해서 전체 코퍼스에 대해 단어 점수표를 계산합니다."
      ],
      "metadata": {
        "id": "EPBOWOYi-CZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_extractor = WordExtractor()\n",
        "word_extractor.train(corpus)\n",
        "word_score_table = word_extractor.extract()\n",
        "# 학습완료"
      ],
      "metadata": {
        "id": "BPwgxYgB-Cc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SOYNLP의 L tokenizer"
      ],
      "metadata": {
        "id": "DS3sVvCn-Cgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어는 띄어쓰기 단위로 나눈 어절 토큰은 주로 L 토큰 + R 토큰의 형식을 가질 때가 많습니다. 예를 들어서 '공원에'는 '공원 + 에'로 나눌 수 있겠지요. 또는 '공부하는'은 '공부 + 하는'으로 나눌 수도 있을 것입니다. L 토크나이저는 L 토큰 + R 토큰으로 나누되, 분리 기준을 점수가 가장 높은 L 토큰을 찾아내는 원리를 가지고 있습니다."
      ],
      "metadata": {
        "id": "8GV1KuHT-CjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
        "l_tokenizer = LTokenizer(scores=scores)\n",
        "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)"
      ],
      "metadata": {
        "id": "DJOP5Wpk-CmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_score_table.items()"
      ],
      "metadata": {
        "id": "kNhKz790-Co3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 최대 점수 토크나이저\n",
        "- 최대 점수 토크나이저는 띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저입니다. 띄어쓰기가 되어 있지 않은 문장을 넣어서 점수를 통해 토큰화 된 결과를 보겠습니다."
      ],
      "metadata": {
        "id": "2Vpwqlzt-QOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "\n",
        "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
        "maxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\")"
      ],
      "metadata": {
        "id": "Jc37uySN-SDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SOYNLP를 이용한 반복되는 문자 정제\n",
        "- SNS나 채팅 데이터와 같은 한국어 데이터의 경우에는 ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우가 많은데 ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요합니다. 이에 반복되는 것은 하나로 정규화시켜줍니다."
      ],
      "metadata": {
        "id": "FwM8odQW-VpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.normalizer import *\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
        "\n",
        "print(repeat_normalize('졔하하하하하하하하하핫', num_repeats=2))\n",
        "print(repeat_normalize('졔하하하하하하핫', num_repeats=2))\n",
        "print(repeat_normalize('졔하하하하핫', num_repeats=2))"
      ],
      "metadata": {
        "id": "kQdyDZqy-WED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customized KoNLPy\n",
        "- 영어권 언어는 띄어쓰기만해도 단어들이 잘 분리되지만, 한국어는 그렇지 않다고 앞에서 몇 차례 언급했었습니다. 한국어 데이터를 사용하여 모델을 구현하는 것만큼 이번에는 형태소 분석기를 사용해서 단어 토큰화를 해보겠습니다. 그런데 형태소 분석기를 사용할 때, 이런 상황에 봉착한다면 어떻게 해야할까요?"
      ],
      "metadata": {
        "id": "1WUDeJHq-Zaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 형태소 분석 입력 : '은경이는 사무실로 갔습니다.'\n",
        "- 형태소 분석 결과 : ['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
      ],
      "metadata": {
        "id": "SJCG_Y_R-Z6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install customized_konlpy"
      ],
      "metadata": {
        "id": "yS8kSmOM-cnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ckonlpy.tag import Twitter\n",
        "twitter = Twitter()\n",
        "twitter.morphs('은경이는 사무실로 갔습니다.')"
      ],
      "metadata": {
        "id": "AUNwpgzp-d7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter.add_dictionary('은경이', 'Noun') # 은 + 경이를 은경이로 토큰화"
      ],
      "metadata": {
        "id": "u-xVCyYB-fWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter.morphs('은경이는 사무실로 갔습니다.')"
      ],
      "metadata": {
        "id": "wHUWn50L-hBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}